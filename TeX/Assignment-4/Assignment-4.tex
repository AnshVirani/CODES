\documentclass{article}
\newcounter{rownumbers}
\newcommand\rownumber{\stepcounter{rownumbers}\arabic{rownumbers}}
%\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{amsmath,amsfonts,mathtools}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\graphicspath{ {E:\College\Sem-6\PSP\CODES\TeX\Assignment-3} }
\usepackage{geometry}
 \geometry{
 a4paper,
 total={210mm,297mm},
 left=20mm,
 right=20mm,
 top=-2mm,
 bottom=2mm,
 }
 
%\usepackage[margin=0.5in]{geometry}

\usepackage{amsmath,amssymb}
\usepackage{ifpdf}
%\usepackage{cite}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{pdfpages}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{parskip}
%\onecolumn
%\input{psfig}
\usepackage{color}
\usepackage{graphicx}
\setlength{\textheight}{23.5cm} \setlength{\topmargin}{-1.05cm}
\setlength{\textwidth}{6.5in} \setlength{\oddsidemargin}{-0.5cm}
\renewcommand{\baselinestretch}{1}
\pagenumbering{arabic}
\linespread{1.15}
\begin{document}
\textbf{
\begin{center}
{
\large{School of Engineering and Applied Science (SEAS), Ahmedabad University}\vspace{5mm}
}
\end{center}
%
\begin{center}
\large{Probability and Stochastic Processes (MAT277)\\ \vspace{4mm}
Homework Assignment-4\\\vspace{2mm}
Enrollment No: AU2140096 \hspace{4cm} Name: Ansh Virani }
\end{center}}
\vspace{2mm}


\vspace{10mm}

\begin{enumerate}
\item 
    To find the probability density function (PDF) of the random variable $Z = aX^2$, where $X$ is a normal random variable with mean 0 and variance $\sigma^2$, and $a > 0$, we'll use the method of transformation of variables.\\
    Let's denote the PDF of $X$ as $f_X(x)$, and the PDF of $Z$ as $f_Z(z)$. We'll first find the cumulative distribution function (CDF) of $Z$ and then differentiate it to obtain the PDF.
    \begin{enumerate}
        \item 
        The CDF of $Z$,
        \[
            F_Z(z) = P(Z \leq z) = P(aX^2 \leq z)
        \]
        Since $a > 0$, we can rewrite this as:
        \[
            F_Z(z) = P(X^2 \leq \cfrac{z}{a})
        \]
        CDF of $X^2$:
        \[
            F_{X^2}(t) = P(X^2 \leq t)
        \]
        Since $X$ is a standard normal random variable, $X^2$ follows a chi-square distribution with one degree of freedom ($\chi^2(1)$).
        \[
            F_{X^2}(t) = P(X^2 \leq t) = P(|X| \leq \sqrt{t})
        \]
        CDF of the standard normal distribution, is given by:
        \[
            \Phi(x) = \cfrac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-\cfrac{u^2}{2}} du
        \]
        So,
        \[
            F_{X^2}(t) = 2 \cdot \Phi(\sqrt{t}) - 1
        \]
    \end{enumerate}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item differentiating to find the PDF:
        \[
            f_Z(z) = \cfrac{d}{dz}F_Z(z)
        \]
        \[
            = \cfrac{d}{dz}\left(2 \cdot \Phi\left(\sqrt{\cfrac{z}{a}}\right) - 1\right)
        \]
        \[
            = \cfrac{1}{\sqrt{\pi}} \cdot \cfrac{1}{2\sqrt{a}} \cdot e^{-\cfrac{z}{2a}} \cdot \cfrac{1}{\sqrt{z}}
        \]
        \[
            = \cfrac{1}{2\sqrt{\pi a}} \cdot \cfrac{1}{\sqrt{z}} \cdot e^{-\cfrac{z}{2a}}
        \]
    \end{enumerate}
    So, the probability density function (PDF) of the random variable $Z = aX^2$ is:
    \[
        f_Z(z) = \cfrac{1}{2\sqrt{\pi a}} \cdot \cfrac{1}{\sqrt{z}} \cdot e^{-\cfrac{z}{2a}}
    \]
    The probability density function (PDF) of the random variable $Z = aX^2$, where $X$ is a normal random variable with mean 0 and variance $\sigma^2$, and $a > 0$, is given by:
    \[
        f_Z(z) = \cfrac{1}{2\sqrt{\pi a}} \cdot \cfrac{1}{\sqrt{z}} \cdot e^{-\cfrac{z}{2a}}
    \]

\newpage
\item 
    A random variable \(X\) is uniformly distributed over the interval (0, 1) and related to \(Y\) by,
    \[
        \tan\left(\cfrac{\pi Y}{2}\right) = e^X \implies Y = \cfrac{2}{\pi}\arctan(e^X)
    \]
    \[
        \therefore\ \ \cfrac{dY}{dX} = \cfrac{2}{\pi}\cdot\cfrac{1}{1+e^{2X}}
    \]
    Applying the the transformation rule, we get:
    \[
        f_Y(y) = f_X(x)\left|\cfrac{dY}{dX}\right| = 1 \times \cfrac{2}{\pi}\cdot\cfrac{1}{1+e^{2X}}
    \]
    Since X is expressed in terms of Y through the initial transformation, $e^X = \tan\left(\cfrac{\pi Y}{2}\right)$, the \(PDF\) can be expressed in terms of Y as follows:
    \[
        f_Y(y) = \left(\cfrac{2}{\pi}\right)\left(\cfrac{1}{1+\tan^2\left(\cfrac{\pi y}{2}\right)}\right)
    \]
    Using the identity $1 + \tan^2(z) = \sec^2(z)$, we get:
    \[
        f_Y(y) = \left(\cfrac{2}{\pi}\right)\left(\cfrac{1}{sec^2\left(\cfrac{\pi y}{2}\right)}\right) = \cfrac{2}{\pi}\cos^2\left(\cfrac{\pi y}{2}\right)
    \]\\
    By solving for \(Y\), computing the derivative with respect to \(X\), and applying the transformation rule, the resulting \(PDF\) for \(Y\) is $f_Y(y) = \cfrac{2}{\pi}\cos^2\left(\cfrac{\pi y}{2}\right)$, valid for \(y\) in the interval $(0, 1)$.
    
\newpage
\item
    Any straight line passing through the point (0, \(l\)) can be represented by the equation $y = mx + l$, where $m$ is the slope of the line.

    From the line equation, we get $x = -\cfrac{l}{m}$.

    Since $m$ can take any real value, the x-intercept can take any real value as well, except $x=0$ (as the line cannot intersect the x-axis at the origin).
    
    As we're drawing the line randomly, we can assume that the probability of the line having any particular slope $m$ is uniformly distributed between negative and positive infinity.

    Therefore, the Probability Density Function (PDF) $f(x)$ is:
    \[
    f(x) = \begin{cases}
    k, & \text{if } x \neq 0\\
    0, & \text{if } x = 0
    \end{cases}
    \]

    Where $k$ is a constant representing the uniform probability density over the entire real line except $x = 0$.

    To find $k$, we can integrate $f(x)$ over its entire range (excluding $x=0$) and set the result equal to 1, since the total probability density over all possible values must equal 1.

    \[
    \int_{-\infty}^{-\epsilon} k\,dx + \int_{\epsilon}^{\infty} k\,dx = 1
    \]
    where $\epsilon$ is a small positive value approaching zero.

    \begin{align*}
    2k\int_{\epsilon}^{\infty} dx &= 1\\
    2k\left[x\right]_{\epsilon}^{\infty} &= 1\\
    2k(\infty - \epsilon) &= 1\\
    2k \cdot \infty &= 1\\
    2k \cdot \infty &\approx 1\\
    k \cdot \infty &\approx \cfrac{1}{2}\\
    k &\approx 0
    \end{align*}

    The constant $k$ represents the uniform probability density over the real line except at $x = 0$. Integrating the probability density function $f(x)$ over its entire range (excluding $x=0$) and setting it equal to 1 yields $k \approx 0$, indicating that $f(x)$ is effectively zero at $x = 0$, consistent with the notion that the line cannot intersect the x-axis at the origin.
    
\newpage
\item
    To find the probability density function (PDF) of the random variable Y given different transformations of the random variable X, we will use the method of transformations.

    Given the probability density function (PDF) of \( X \) as:

    \[ 
        f_X(x) = \cfrac{1}{\pi(1 + x^2)}
    \]
    \begin{enumerate}
        \item 
        \( \mathbf{Y = 1 - X^3}\)\\
        We start by finding the cumulative distribution function (CDF) of Y and then differentiate it to get the PDF of Y.

        i. Finding the CDF of Y:
        \[ 
            F_Y(y) = P(Y \leq y) = P(1 - X^3 \leq y) 
        \]
        Solve for X:
        \[ 
            X \leq (1 - y)^{1/3} 
        \]
        \[ 
            F_Y(y) = P(X \leq (1 - y)^{1/3}) 
        \]
        \[
            F_Y(y) = \int_{-\infty}^{(1 - y)^{1/3}} \cfrac{1}{\pi(1 + x^2)} dx
        \]
        Let $ u = 1 + x^2 $, then $ du = 2x dx $, and $ dx = \cfrac{du}{2x} $.
        The integral becomes:
        \[
            F_Y(y) = \cfrac{1}{2\pi} \int_{2}^{\cfrac{1}{(1 - y)^{2/3}}} \cfrac{1}{u} du
        \]
        \[
            = \cfrac{1}{2\pi} \ln|u| \bigg|_{2}^{\cfrac{1}{(1 - y)^{2/3}}}
        \]
        \[
            = \cfrac{1}{2\pi} \ln\left(\cfrac{1}{(1 - y)^{2/3}}\right) - \cfrac{1}{2\pi} \ln(2)
        \]
        \[
            = -\cfrac{1}{2\pi} \ln(1 - y) - \cfrac{1}{3\pi} \ln(2)
        \]
        ii. Finding the PDF of $Y$:

        differentiating the CDF $F_Y(y)$ with respect to $y$, we get the PDF $f_Y(y)$:
        \[
            f_Y(y) = \cfrac{d}{dy} F_Y(y)
        \]
        \[
            = -\cfrac{1}{2\pi} \left( -\cfrac{1}{1 - y} \right)
        \]
        \[
            = \cfrac{1}{2\pi(1 - y)}
        \]
        \item \( \mathbf{Y = arctan(X)} \)\\
        Similar to the previous transformation, we find the CDF and then differentiate to get the PDF.

        i. Finding the CDF of Y:
        \[ 
            F_Y(y) = P(Y \leq y) = P(\arctan(X) \leq y) 
        \]
        \[ 
            = P(X \leq \tan(y))
        \]
        \[
            F_Y(y) = \int_{-\infty}^{\tan(y)} \cfrac{1}{\pi(1 + x^2)} dx
        \]
        This integral can be recognized as the inverse tangent function:
        \[
            F_Y(y) = \cfrac{1}{\pi} \left[ \arctan(\tan(y)) - \arctan(-\infty) \right]
        \]
        \[
            F_Y(y) = \cfrac{1}{\pi} \left[ y - \left( -\cfrac{\pi}{2} \right) \right]
        \]
        \[
            F_Y(y) = \cfrac{1}{\pi} \left( y + \cfrac{\pi}{2} \right)
        \]
        ii. Finding the PDF of $Y$:

        differentiating the CDF $F_Y(y)$ with respect to $y$ to get the PDF $f_Y(y)$:

        \[
            f_Y(y) = \cfrac{d}{dy} F_Y(y)
        \]
        \[
            = \cfrac{1}{\pi}
        \]
        Hence, for $Y = \arctan(X)$, the PDF of Y is a constant function with value $\cfrac{1}{\pi}$ within the interval where $-\cfrac{\pi}{2} < y < \cfrac{\pi}{2}$. Outside of this interval, the PDF is zero.
        \end{enumerate}

\newpage
\item 
    Given $X$ is a random variable on $(0, \infty)$ with pdf
    \[
        f(x) = e^{-x}, \quad x \in (0, \infty)
    \]
    Now given $Y$ is a random variable on $(0, \infty)$ such that
    \[ 
        Y = X^2
    \]
    \[
        X = \sqrt{Y}
    \]
    \[
        \cfrac{dx}{dy} = \cfrac{1}{2\sqrt{y}}
    \]
    and 
    \[
        f(y) = e^{-\sqrt{y}}, \quad y \in (0, \infty)
    \]
    So \(PDF\) for $Y$,
    \[
        f_Y(y) = f(y) \left|\cfrac{dx}{dy}\right|
    \]
    \[
        f_Y(y) = e^{-\sqrt{y}} \cdot \cfrac{1}{2\sqrt{y}}
    \]
    \[
        f_Y(y) = \cfrac{1}{2} \cdot \cfrac{e^{-\sqrt{y}}}{\sqrt{y}}, \quad y \in (0, \infty)
    \]

\newpage
\item 
    To evaluate the probability density function (pdf) of the random variable X given by the expression:
    \[
        x = \cfrac{1}{2}\left[1 + \cfrac{2}{\sqrt{2\pi}}\int_{0}^{Y -\  \theta} \exp\left(-\cfrac{t^2}{2}\right) dt\right]
    \]
    \[
        g(t) = e^{-\cfrac{t^2}{2}}
    \]
    To find the \(PDF\) $f(x)$, we differentiate the given expression with respect to Y (as X is dependent on Y) using the chain rule:
    \[
        f(Y) = \cfrac{dX}{dY}
    \]
    \[
        f(Y) = \cfrac{1}{2}\times \cfrac{1}{\sqrt{2\pi}} \times e^{-\cfrac{\left(\cfrac{Y - \theta}{\sigma_y}\right)^2}{2}} \times \cfrac{1}{\sigma_y}
    \]
    \[
        f(Y) = \cfrac{1}{\sigma_y\sqrt{2\pi}} \times e^{-\cfrac{\left(\cfrac{Y - \theta}{\sigma_y}\right)^2}{2}}
    \]\\
    This is the Probability Density Function (PDF) of the random variable \(Y\).

\newpage
\item

\newpage
\item

\newpage
\item
    \textbf{(a): Probability density \( f(x, y) \) of the system of random variables \( (X, Y) \) is given:}

    Given the joint probability density function \( f(x, y) \) for \( (X, Y) \), we need to find the probability density function (PDF) of \( Z = \cfrac{X}{Y} \).
    The PDF of \( Z \) is given by:
    \[ 
        f_Z(z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} |J| \cdot f(x, y) \,dx \,dy 
    \]
    where \( J \) is the Jacobian determinant.

    Since the joint probability density function \( f(x, y) \) is given, let's denote it as \( f_{X,Y}(x, y) \). Then, we have:
    \[ 
        f_{X,Y}(x, y) = f(x, y) 
    \]
    The Jacobian determinant \( J \) is calculated as:
    \[ 
        J = \left| \cfrac{\partial(x, y)}{\partial(z)} \right| 
    \]
    For the transformation \( Z = \cfrac{X}{Y} \), we have:
    \[ Z = \cfrac{X}{Y} \]
    \[ X = ZY \]
    Taking partial derivatives with respect to \( X \) and \( Y \), we get:
    \[ 
        \cfrac{\partial X}{\partial Z} = Y 
    \]
    \[ 
        \cfrac{\partial X}{\partial Y} = Z 
    \]
    Therefore, the Jacobian determinant \( J \) is:
    \[ 
        J = \left| \cfrac{\partial(x, y)}{\partial(z)} \right| = \left| \cfrac{\partial(X, Y)}{\partial(Z)} \right| = |YZ| = |ZY| = |Z| 
    \]
    The PDF of \( Z \) is given by:
    \[
        f_Z(z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} |Z| \cdot f_{X,Y}(x, y) \,dx \,dy
    \]
    \[
         = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} |z| \cdot f(x, y) \,dx \,dy
    \]
    \[ 
        = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} |z| \cdot f(x, y) \,dx \,dy 
    \]
    \newpage
    \textbf{(b): X and Y are independent random variables obeying Rayleigh’s distribution law:}

    Given that \( X \) and \( Y \) are independent random variables obeying Rayleigh's distribution, we have the probability density functions:
    \[ 
        f_X(x) = \begin{cases} \cfrac{x}{a^2} \exp\left(-\cfrac{x^2}{2a^2}\right) & \text{for } x \geq 0 \\ 0 & \text{for } x \leq 0 \end{cases} \]

    \[ 
        f_Y(y) = \begin{cases} \cfrac{y}{a^2} \exp\left(-\cfrac{y^2}{2a^2}\right) & \text{for } y \geq 0 \\ 0 & \text{for } y \leq 0 \end{cases} \]

    Since \( X \) and \( Y \) are independent, their joint PDF is the product of their individual PDFs:
    \[ 
        f_{X,Y}(x, y) = f_X(x) \cdot f_Y(y) 
    \]
    \[ 
        = \begin{cases} \cfrac{x}{a^2} \exp\left(-\cfrac{x^2}{2a^2}\right) \cdot \cfrac{y}{a^2} \exp\left(-\cfrac{y^2}{2a^2}\right) & \text{for } x \geq 0, y \geq 0 \\ 0 & \text{otherwise} \end{cases}
    \]
    \[ 
        = \begin{cases} \cfrac{xy}{a^4} \exp\left(-\cfrac{x^2 + y^2}{2a^2}\right) & \text{for } x \geq 0, y \geq 0 \\ 0 & \text{otherwise} \end{cases} 
    \]\\
    \[ 
        f_{X,Y}(x, y) = \cfrac{xy}{a^4} \exp\left(-\cfrac{x^2 + y^2}{2a^2}\right)
    \]
    \[ 
        f_Z(z) = \int_{0}^{\infty} \int_{0}^{\infty} |z| \cdot \cfrac{xy}{a^4} \exp\left(-\cfrac{x^2 + y^2}{2a^2}\right) \,dx \,dy
    \]
    First, let's integrate with respect to \( x \):
    \[ 
        \int_{0}^{\infty} \cfrac{xy}{a^4} \exp\left(-\cfrac{x^2 + y^2}{2a^2}\right) \,dx
    \]
    Let's substitute \( u = x^2 + y^2 \), then \( du = 2x \,dx \).
    \[ 
        \cfrac{1}{2} \int_{0}^{\infty} \cfrac{1}{a^4} e^{-u/(2a^2)} \,du 
    \]
    \[ 
        = -\cfrac{1}{2} \left[ e^{-u/(2a^2)} \right]_{0}^{\infty} = -\cfrac{1}{2} \left(0 - 1\right) = \cfrac{1}{2} 
    \]
    Now, let's integrate with respect to \( y \) from 0 to \( \infty \):
    \[ 
        f_Z(z) = |z| \cdot \cfrac{1}{2} \cdot \int_{0}^{\infty} \,dy
    \]
    \[ 
        = \cfrac{|z|}{2} \cdot \left[y\right]_{0}^{\infty} = \cfrac{|z|}{2} \cdot \left(\infty - 0\right) = \infty
    \]\\
    Therefore, we can state that the resulting PDF \( f_Z(z) \) is not properly normalized. It appears that the integral diverges, indicating that the PDF \( f_Z(z) \) does not exist.

\newpage
\item
    \textbf{(a) Probability density $f(x, y)$ for the system of random variables $(X, Y)$ is given:}

    If the joint probability density function $f(x, y)$ is given, we can directly compute the PDF of $R$ using the transformation method.

    Given $R = \sqrt{X^2 + Y^2}$, the Jacobian determinant of the transformation is $\cfrac{\partial(x, y)}{\partial(r)} = \cfrac{r}{\sqrt{x^2 + y^2}}$.

    So, the PDF of $R$ is:
    \[ 
        f_R(r) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \cfrac{1}{r} f(x, y) \,dx \,dy
    \]
    \textbf{(b) Random variables $X$ and $Y$ are independent and obey the same normal distribution $N(0, \sigma)$:}

    Given independent normal distributions for $X$ and $Y$, we can exploit the fact that the sum of squares of independent standard normal variables follows a chi-squared distribution.

    Since $X$ and $Y$ are independent, $X^2$ and $Y^2$ are also independent. Therefore, $R^2 = X^2 + Y^2$ follows a chi-squared distribution with 2 degrees of freedom, which is equivalent to an exponential distribution with parameter $\cfrac{1}{2\sigma^2}$.
    \[ 
        f_R(r) = \cfrac{r}{\sigma^2}\cdot e^{-\left(\cfrac{r^2}{2\sigma^2}\right)} 
    \]
    \textbf{(c) Random variables $X$ and $Y$ are independent normal random variables with probability density $f(x, y)$:}

    Given that the joint probability density function \( f(x, y) \) for the system of random variables \( (X, Y) \) is:

    \[ 
        f(x, y) = \cfrac{1}{2\pi\sigma^2} e^{-\cfrac{(x - h)^2 + y^2}{2\sigma^2}} 
    \]
    We want to find the probability density function (PDF) for the modulus of the radius vector \( R = \sqrt{X^2 + Y^2} \).

    We'll use the transformation method. The transformation is \( R = \sqrt{X^2 + Y^2} \). To find the PDF of \( R \), we need to calculate:
    \[ 
        f_R(r) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \cfrac{1}{r} f(x, y) \,dx \,dy 
    \]
    Substituting the given expression for \( f(x, y) \), we have:
    \[ 
        f_R(r) = \cfrac{1}{2\pi\sigma^2r} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-\cfrac{(x - h)^2 + y^2}{2\sigma^2}} \,dx \,dy 
    \]
    \[ 
        = \cfrac{1}{2\pi\sigma^2r} \int_{-\infty}^{\infty} e^{-\cfrac{(x - h)^2}{2\sigma^2}} \left( \int_{-\infty}^{\infty} e^{-\cfrac{y^2}{2\sigma^2}} \,dy \right) \,dx 
    \]
    The inner integral \( \int_{-\infty}^{\infty} e^{-\cfrac{y^2}{2\sigma^2}} \,dy \) is simply the integral of a standard normal distribution, and it equals \( \sqrt{2\pi\sigma^2} \).
    \[ 
        = \cfrac{1}{2\pi\sigma^2r} \int_{-\infty}^{\infty} e^{-\cfrac{(x - h)^2}{2\sigma^2}} \cdot \sqrt{2\pi\sigma^2} \,dx
    \]
    \[ 
        = \cfrac{1}{r} \int_{-\infty}^{\infty} e^{-\cfrac{(x - h)^2}{2\sigma^2}} \,dx
    \]
    Now, we have an integral of a Gaussian function, which integrates to \( \sqrt{2\pi\sigma^2} \).
    \[ 
        f_R(r) = \cfrac{1}{r} \cdot \sqrt{2\pi\sigma^2} 
    \]
    \[ 
        f_R(r) = \cfrac{\sqrt{2\pi\sigma^2}}{r}
    \]
    \textbf{(d) Random variables $X$ and $Y$ are independent normal random variables with mean $\mu_x = \mu_y = 0$ and variances $\sigma_x^2$ and $\sigma_y^2$, respectively:}

    Since $X$ and $Y$ are independent, their squares $X^2$ and $Y^2$ are also independent. Therefore, $R^2 = X^2 + Y^2$ follows a chi-squared distribution with 2 degrees of freedom.

    The PDF of $R$ will be similar to case (b):
    \[ 
        f_R(r) = \cfrac{r}{\sigma_x \sigma_y}\cdot e^{-\left(\cfrac{r^2}{2(\sigma_x^2 + \sigma_y^2)}\right)}
    \]

\newpage
\item
    \textbf{We have the quadratic equation:
        \[
            x^2 + \alpha x + \beta = 0,
        \]
        whose both roots take all values from -1 to +1 with equal probabilities.}\\
        Now, let's denote the roots of the quadratic equation as $r_1$ and $r_2$.\\
        From the quadratic formula, we have:
        \begin{align}
            r_1 =\cfrac{-\alpha + \sqrt{\alpha^2 - 4\beta}}{2}\\
            r_2 =\cfrac{-\alpha - \sqrt{\alpha^2 - 4\beta}}{2}
        \end{align}
        We know that both roots can take all values from -1 to +1 with equal probabilities. Since the roots are symmetric to the coefficient $\alpha$, we can assume that $r_1$ takes values from -1 to +1 with equal probabilities, and so does $r_2$. This means that the distribution of the sum and the product of the roots are uniform.
        \begin{align}
            r_1 + r_2 = -\alpha\\
            r_1^\cdot r_2 = \beta
        \end{align}
        Therefore, the probability density function of \(\alpha\) is given by the distribution of the sum of two independent uniform random variables between - 1 and +1, which is a triangular distribution with the density.
        \[
            f(\alpha) = \begin{cases}
            1 - |\alpha|, & \text{if } -1\le \alpha \le1\\
            0, & \text{otherwise}
            \end{cases}
        \]
        Similarly, the probability density function of $\beta$ is given by the distribution of the product of two Independent uniform random variables between -1 and +1 which is a distribution that peaks at zero and has a maximum density of 3/4 at $\beta = 0$.
        \[
            g(\beta) = \begin{cases}
            \cfrac{3}{4} (1 - \beta^2), & \text{if } -1\le \beta \le1\\
            0, & \text{otherwise}
            \end{cases}
        \]
        
\end{enumerate}
\end{document}